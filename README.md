# 100DaysOfCode-Challenge
I've as of 30th June 2018 started this challenge where I will code for next 100 days on some exiting projects related to AI and ML.

You should definitely check it out http://www.100daysofcode.com/ .

I'll update all of the work that I'm doing on a daily basis here in this repository. 

I'm wuite pumped up to start with this challenge. Although I'm quite busy with my ongoing online nanodegrees and another AI related freelancing work. 

The projects I'll be taking on will be completely new and fresh and something related above work won't count.

### Day1
I've started participating in **online kaggle competitions**. For now I've decided to go with a very introductory problem that will give me a bit of exposure about the kaggle environment and workspace in general.  

I'll be working on **Home Credit Default Risk**. You can read more about the challenge on this [(https://www.kaggle.com/c/home-credit-default-risk. )]

Now working on kaggle competitions can be highly overwhelming and looks very daunting at first. So I've forked one of the notebooks that can get me quickly started. For later projects, I'll be doing things from *level zero*. 

### Day2 
As I moved from the understanding the problem statement to code, I observed few things. 

The preprocessing steps are quite important and kind of tough to crack. A lot of machine learning problems have do not have datasets available easily. So getting started means you have to get the data first. You may scrape the data using various services available on net or write you own script using libraries like Beautiful Soup. 

Even if they have datasets, you have to do a lot of preproccessing otherwise you won't land anywhere. You know all about machine learning and deep learning, but you don't know how to exploit data, you're useless just like a knight with the sharpest sword in a deserted place. 

In ******Home Credit Default Risk****** problem there are a lot of files total 7 to be precise. I've started working on applications_train.csv file. This file contains total 307511 data points with total 121 columns or features corresponding to every datapoint. Like I've said some of the entries in some columns are missing, so I'm trying to somehow understand and find a solution to this problem. Since I don't know what features will be beneficial or important for the model, I cannot simply remove the entries that are missing(Imputing). 
